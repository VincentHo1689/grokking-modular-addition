# grokking-modular-addition
This report aims to investigate the learning of binary and multivariate modular addition operations and explore the impact of different factors on the Grokking phenomenon. Experiments employ deep learning models, particularly Transformers, Multilayer Perceptrons (MLPs), and Long Short-Term Memory networks (LSTMs), combined with various optimizers and regularization techniques, to learn modular addition operations. Experimental results demonstrate that under different network architectures, as the data fraction a varies, most models exhibit a pronounced Grokking phenomenon, where after initially overfitting, the accuracy of the validation set suddenly improves significantly, ultimately achieving perfect generalization. Furthermore, the study finds that different optimizers and regularization techniques have a significant impact on the Grokking phenomenon, with the AdamW optimizer combined with appropriate weight decay and Dropout rates exhibiting strong generalization ability and robustness. Meanwhile, for multivariate modular addition operations, as the number of summand terms K increases, the complexity and difficulty of the problem gradually rise, requiring more training epochs to achieve the desired performance level. Finally, this report discusses the mechanism of the Grokking phenomenon, its influencing factors, and future research directions, providing new perspectives and ideas for understanding the essence of deep learning and developing more efficient and generalized neural network models.
